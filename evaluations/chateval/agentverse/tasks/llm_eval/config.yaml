task:
  llmeval
data_path:
  ./test_data/test.json
output_dir:
  ./outputs/diypc
prompts:
  prompt: &prompt |-
    [Question]
    ${source_text}
    [The Start of Assistant 1’s Answer]
    ${compared_text_one}
    [The End of Assistant 1’s Answer]
    [The Start of Assistant 2’s Answer]
    ${compared_text_two}
    [The End of Assistant 2’s Answer]
    [System]
    We hope you can provide your feedback and opinions on the performance of the two AI assistants’ responses to the user’s question shown above.
    When evaluating, please pay special attention to the criteria related to recommended PC build lists, 
    such as the practicality and performance of the suggested components, the balance between cost and efficiency, 
    compatibility and potential for future upgrades, the completeness of the detailed explanations, and how well they meet the user’s needs.
    Additionally, since several other reviewers have been assigned the same task, 
    you must discuss with them and think carefully before making your final judgment.
    Each assistant’s overall performance should be rated on a scale of 1 to 10, with higher scores indicating better overall performance.

    ${role_description}

    Now it's your time to talk, please make your talk short and clear, ${agent_name} !

    ${final_prompt}


environment:
  env_type: llm_eval
  max_turns: 4
  rule:
    order:
      type: sequential
    visibility:
      type: all
    selector:
      type: basic
    updater:
      type: basic
    describer:
      type: basic

agents:
  -
    agent_type: llm_eval_multi
    name: Hard Critic
    final_prompt_to_use: |-
      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
      Then, output two lines indicating the scores for Assistant 1 and 2, respectively.

      Remember that you are not required to output the same value as other referees !
      Output with the following format strictly:
      Evaluation evidence: [your explanation here]
      The score of Assistant 1: [score only]
      The score of Assistant 2: [score only]
    role_description: |-
      You are now Hard Critic, one of the referees in this task. When evaluating PC build recommendation lists, you focus on whether they meet the purpose of the question, fit within the given budget, consider if the power consumption is too high or insufficient, and evaluate the quality of the cooling solution. Please think critically by yourself and note that it's your responsibility to choose one of which is the better first.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-4"
      llm_type: gpt-4
      temperature: 0
      max_tokens: 512
  -
    agent_type: llm_eval_multi
    name: Soft Critic
    final_prompt_to_use: |-
      Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
      Then, output two lines indicating the scores for Assistant 1 and 2, respectively.

      Remember that you are not required to output the same value as other referees !
      Output with the following format strictly:
      Evaluation evidence: [your explanation here]
      The score of Assistant 1: [score only]
      The score of Assistant 2: [score only]
    role_description: |-
      You are now Soft Critic, one of the referees in this task. When evaluating PC build recommendation lists, you focus on criteria such as component longevity, warranty coverage, aesthetics, brand reputation, and energy efficiency. Please think critically on your own and remember that it is your responsibility to decide which recommendation stands out as superior.
    memory:
      memory_type: chat_history
    memory_manipulator:
      memory_manipulator_type: basic
    prompt_template: *prompt
    llm:
      model: "gpt-4"
      llm_type: gpt-4
      temperature: 0
      max_tokens: 512

tools: ~